# -*- coding: utf-8 -*-
"""mobile_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1So10AcJ9FpljXCWSI71YUFGncz9aT_UW

##Importing libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from scipy.stats import boxcox
from scipy.special import inv_boxcox

"""## Data gathering"""

file_path = '/content/Cellphone.csv'
data = pd.read_csv(file_path)
data

# checking data info
data.info()

# checkimg for null values
data.isnull().sum()

"""### **Data preprocessing**"""

# removing unwanted columns
df = data.copy()
df.drop('Product_id', axis=1, inplace=True)
df.head()

# renaming column names
df.rename(columns = {'Price': 'price', 'Sale': 'sale','resoloution': 'resolution', 'ppi': 'phone_pixel_density', 'cpu core': 'cpu_core',
                     'cpu freq': 'cpu_freq', 'internal mem': 'int_memory', 'RearCam': 'rear_cam', 'Front_Cam': 'front_cam',}, inplace = True)

df.columns

"""### Fetching relationship between columns"""

df_corr = df.corr()
df_corr

plt.figure(figsize=(8,6))
sns.heatmap(df_corr, annot = True, cmap = 'RdYlGn')
plt.show()

df_corr.price

# selecting columns based on correlation threshold value
corr_thres = 0.5
df_corr_price = df_corr['price'][abs(df_corr['price']) > corr_thres]
df_corr_price

df[df_corr_price.index].describe()

"""### Checking the overall distribution of all columns"""

for col in df_corr_price.index:
  plt.figure(figsize = (4,2))
  plt.hist(df[col], bins = 20)
  plt.title(col)
  plt.show()

"""Most of the features in the dataset are either skewed or sparse, as evident from the output of the describe() method, where some features exhibit extremely high standard deviations.

To address this, we can apply transformation techniques to normalize or stabilize the distribution of these features. In this case, I used log transformation for most features and square root transformation for columns containing zero values to ensure meaningful adjustments.
"""

df_corr_price.index

# splitting dataset into features and target variables
y = 'price'
Y = df[y].copy()
X = df[df_corr_price.index[1:]].copy()

Y.head()

X.describe()

battery_log = np.log(X['battery'])

ppi_log = np.log(X['phone_pixel_density'])

thickness_log = np.log(X['thickness'])

int_memory_sqrt = np.sqrt(X['int_memory'])

rear_cam_sqrt = np.sqrt(X['rear_cam'])

front_cam_sqrt = np.sqrt(X['front_cam'])

X_new = pd.DataFrame({'battery_log': battery_log, 'ppi_log': ppi_log,
                      'thickness': thickness_log, 'cpu_core': X['cpu_core'],
                      'cpu_freq': X['cpu_freq'], 'int_memory': int_memory_sqrt,
                      'ram': X['ram'], 'rear_cam_sqrt': rear_cam_sqrt,
                      'front_cam_sqrt': front_cam_sqrt})

"""### Data modelling"""

y_bc, y_lam = boxcox(Y)

s = StandardScaler()
poly = PolynomialFeatures(degree = 2)
lr = LinearRegression()

X_new.shape

x_poly = poly.fit_transform(X_new)
x_std = s.fit_transform(x_poly)
x_std.shape

x_train, x_test, y_train, y_test = train_test_split(x_std, y_bc, test_size = 0.2, random_state = 42)

model = lr.fit(x_train, y_train)
y_test_pred = model.predict(x_test)

r2_test = r2_score(y_test, y_test_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
print(r2_test)
print(mse_test)

y_train_pred = model.predict(x_train)
mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
print(r2_train)
print(mse_train)

plt.figure(figsize = (12,6))

plt.subplot(1,2,1)
plt.plot(y_test, y_test_pred, 'o')
plt.title('Predicted vs Actual Values (Test data)')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')

plt.subplot(1,2,2)
plt.plot(y_train, y_train_pred, 'o')
plt.title('Predicted vs Actual Values (Train data)')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')

plt.tight_layout()
plt.show()

rmse_train = np.sqrt(mse_train)
rmse_train

rmse_test = np.sqrt(mse_test)
rmse_test

y_test_org = inv_boxcox(y_test_pred, y_lam)

"""The results indicate that the predicted values closely match the actual values in both the training and test datasets. This is evident from the visualizations, which highlight the model's ability to capture the relationship between the features and the target variable with high accuracy.

However, it is important to note that the linear regression model has been trained on a relatively small dataset. While the current performance appears satisfactory, training the model on a larger dataset would likely improve its accuracy and generalizability. Expanding the dataset would provide the model with a more diverse range of patterns, reducing potential overfitting and enhancing its ability to make reliable predictions on new data.
"""