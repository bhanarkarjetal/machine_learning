# -*- coding: utf-8 -*-
"""comparison of different models in linear regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gd7AyVyitBFjYA6-0ru62Do3o9OvgiF8

## Importing necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from scipy.stats import boxcox

"""## Data gathering"""

data = pd.read_excel('stock_market_dataset.xlsx')

data.head()

data.columns

"""## Data preprocessing"""

# changing column name format for uniformity
data.columns = data.columns.str.lower().str.replace('*','').str.replace(' ','_')

data.columns

# changing datatype of 'date' column for further calculation
data['date'] = pd.to_datetime(data['date'])

# getting 'weekday name', 'month name' and 'year' from 'date' columns
data['wkday'] = data['date'].dt.day_name()
data['month'] = data['date'].dt.month_name()
data['year'] = data['date'].dt.year

data.head()

data.drop(['date'], axis=1, inplace=True)
data.head()

data.dtypes

data.shape

"""## Statistical analysis"""

data_corr = data.select_dtypes(include = np.number).corr()
data_corr

# fetching relevant data for analysis based on correlation values
corr_threshold = 0.7
new_cols = []
close_corr = data_corr['close']
close_corr.drop(['close'],inplace = True)
for i in close_corr.index:
    if close_corr[i] > corr_threshold:
        new_cols.append(i)
new_cols

data_stat = data[new_cols].describe()
data_stat

"""## Features and target variables selection"""

# setting target variable
y_col ='close'

# getting numerical columns
x_num = new_cols

# getting categorical columns
x_cat = data.select_dtypes(exclude=np.number).columns
x_cat

X_num = data[x_num]
X_cat = data[x_cat]
y = data[y_col]

X_num.head()

# visualizing categorical features
fig, ax = plt.subplots(1,2, figsize = (10,4))

sns.countplot(x = 'wkday', data = data, ax = ax[0])
sns.countplot(x = 'month', data = data, ax = ax[1])
plt.xticks(rotation = 45)

plt.tight_layout()
plt.show()

# one hot encoding for categorical variables
data_encode = pd.get_dummies(X_cat, drop_first = True).astype(int)
data_encode.head()

data_encode.shape

# visualizing numerical features
fig, ax = plt.subplots(1,5, figsize = (20,4))
sns.histplot(x = 'open', data = data, ax = ax[0], kde = True)
sns.histplot(x = 'high', data = data, ax = ax[1], kde = True)
sns.histplot(x = 'low', data = data, ax = ax[2], kde = True)
sns.histplot(x = 'volume', data = data, ax = ax[3], kde = True)
sns.histplot(x = 'adj_close', data = data, ax = ax[4], kde = True)

plt.tight_layout()

"""## Data transformation"""

X_log = np.log(X_num)
X_log.head()

X_log.describe()

fig, ax = plt.subplots(1,4, figsize = (20,4))
sns.histplot(x = 'open', data = X_log, ax = ax[0], kde = True)
sns.histplot(x = 'high', data = X_log, ax = ax[1], kde = True)
sns.histplot(x = 'low', data = X_log, ax = ax[2], kde = True)
sns.histplot(x = 'adj_close', data = X_log, ax = ax[3], kde = True)

plt.tight_layout()

X_new = pd.concat([X_num, data_encode], axis = 1)
X_new.head()

X_new_log = pd.concat([X_log, data_encode], axis = 1)
X_new_log.head()

y.describe()

y_bc, y_lam = boxcox(y)

"""## Linear Regression model

The main objective of this project is to compare the coefficients and errors using different transformation and regularization methods. So, for this analysis, I will use the dataset: X_new_log and y_bc, which is log transformation of features (X_new_log) and bixcox_transformation of target variable (y_bc).

Case 1: Use standard scaler.

Case 2: Use standard scaler with Polynomial features of degree 2.

Case 3: Use standard scaler, polynomial feature with Lasso regularization (using cross validation method to find the best value of alpha)

Case 4: Use standard scaler, polymonial feature with Ridge regularization (using cross validation method to find the best value of alpha)
"""

# creating instances of linear regression model, and scaling and polynomial features
lr = LinearRegression()
std = StandardScaler()
poly = PolynomialFeatures(degree = 2, include_bias = False)

# creating instances for Lasso and Ridge Regression
lasso = Lasso()
ridge = Ridge()

# splitting dataset into training and testing sets with test-size = 0.2
X_train, X_test, y_train, y_test = train_test_split(X_new_log, y_bc, test_size = 0.2, random_state = 42)

# case 1: using standard scaler
X_train_std = std.fit_transform(X_train)
X_test_std = std.transform(X_test)

lr.fit(X_train, y_train)
y_pred_test = lr.predict(X_test)
y_pred_train = lr.predict(X_train)

mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

print('Train MSE: ', mse_train)
print('Test MSE: ', mse_test )

print('Train R2: ', r2_train)
print('Test R2: ', r2_test)

index_val = ['Train MSE', 'Test MSE', 'Train R2', 'Test R2']

error_df = pd.DataFrame({'standard_scaler': [mse_train, mse_test, r2_train, r2_test]}, index = index_val)
error_df.head()

# fetching coefficients of linear regression model
coef_lr = lr.coef_

# creating a dataframe , 'coef_df', of coefficients
coef_df = pd.DataFrame(coef_lr, index = X_new_log.columns, columns = ['coef'])
coef_df.sort_values(by = 'coef', ascending = False).head()

# case 2: using standard scaler and polynomial feature
X_train_poly = poly.fit_transform(X_train_std)
X_test_poly = poly.transform(X_test_std)

X_train_std = std.fit_transform(X_train_poly)
X_test_std = std.transform(X_test_poly)

lr.fit(X_train_std, y_train)
y_pred_test = lr.predict(X_test_std)
y_pred_train = lr.predict(X_train_std)

mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

print('Train MSE: ', mse_train)
print('Test MSE: ', mse_test )

print('Train R2: ', r2_train)
print('Test R2: ', r2_test)

error_df['scaling_and_polynomial'] =  [mse_train, mse_test, r2_train, r2_test]

error_df

# get column names after applying polynomial features
poly_cols = poly.get_feature_names_out(X_new_log.columns)
coef_poly = lr.coef_

# creating a dataframe for coefficients
coef_comparison = pd.DataFrame({'coef_poly': coef_poly}, index = poly_cols)
coef_comparison.sort_values(by = 'coef_poly', ascending = False).head()

# case 3: standard scaler, polynomial feature with Lasso
lasso = Lasso()

# Define parameter grid for cross validation
param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}

# Grid Search
grid = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)
grid.fit(X_train, y_train)

# finding best alpha value for Lasso
best_alpha = grid.best_params_['alpha']
print("Best alpha for Lasso:", best_alpha)

# using best alpha value to predict target variable
lasso = Lasso(alpha = best_alpha)
lasso.fit(X_train_std, y_train)

y_pred_test = lasso.predict(X_test_std)
y_pred_train = lasso.predict(X_train_std)

mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

print('Train MSE: ', mse_train)
print('Test MSE: ', mse_test )

print('Train R2: ', r2_train)
print('Test R2: ', r2_test)

error_df['lasso'] =  [mse_train, mse_test, r2_train, r2_test]

# fetching coefficients for lasso regression
coef_lasso = lasso.coef_

# adding lasso coefficients to coef_comparison dataframe
coef_comparison['coef_lasso'] = coef_lasso

# case 4: standard sclaer, polynomial feature with Ridge
ridge = Ridge()

# Define parameter grid
param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}

# Grid Search
grid = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=5)
grid.fit(X_train, y_train)

# finding best alpha value for Ridge
best_alpha = grid.best_params_['alpha']
print("Best alpha for Ridge:", best_alpha)

# using best alpha value to predict target variable
ridge = Ridge(alpha = best_alpha)
ridge.fit(X_train_std, y_train)

y_pred_test = ridge.predict(X_test_std)
y_pred_train = ridge.predict(X_train_std)

mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

print('Train MSE: ', mse_train)
print('Test MSE: ', mse_test )

print('Train R2: ', r2_train)
print('Test R2: ', r2_test)

error_df['ridge'] =  [mse_train, mse_test, r2_train, r2_test]

# fetching coefficients for Ridge regression
coef_ridge = ridge.coef_

# adding ridge coefficient to coef_comparison dataframe
coef_comparison['coef_ridge'] = coef_ridge

"""## Analysis comparison"""

coef_comparison.sort_values(by = 'coef_poly', ascending = False).head()

error_df

fig, ax = plt.subplots(1,4, figsize = (12,3))

x_val_lr = np.linspace(coef_lr.min(), coef_lr.max(), len(coef_lr))
ax[0].plot(x_val_lr, coef_lr)
plt.title('Linear Regression')

x_val_poly = np.linspace(coef_poly.min(), coef_poly.max(), len(coef_poly))
ax[1].plot(x_val_poly, coef_poly)
plt.title('Polynoimal features')

x_val_lasso = np.linspace(coef_lasso.min(), coef_lasso.max(), len(coef_lasso))
ax[2].plot(x_val_lasso, coef_lasso)
plt.title('Lasso Regression')

x_val_ridge = np.linspace(coef_ridge.min(), coef_ridge.max(), len(coef_ridge))
ax[3].plot(x_val_ridge, coef_ridge)
plt.title('Ridge Regression')

fig.suptitle('Coefficients comparison')
plt.tight_layout()